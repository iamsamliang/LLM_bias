This study explores the presence of biases present in large language models when surveying humans. These biases include majority label, recency, and common token biases. We designed a survey using inspiration from prompts given to the GPT-3 language model in Zhao et al.’s 2021 paper ”Calibrate Before Use: Improving Few-Shot Performance of Language Models.” The questions were modified for human subjects by using questions with multiple correct answers and randomization of symbols to avoid biases associated with certain answers. The survey found that common token and recency biases were more prevalent than majority label bias in humans, though all were present to a degree. The survey had certain limitations such as prompt format length and repetition, question order, biases in the sample tested, and sub-optimal prompts for GPT-3. Still, this study presents preliminary results that shed light on how surveys of this nature can be improved and how large language models can better match human behaviors.

Full paper is above titled paper.pdf
